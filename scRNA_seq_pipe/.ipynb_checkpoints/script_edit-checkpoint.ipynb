{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis script is to modify new script\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "this script is to modify new script\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load rm_dup_single_barcode_UMI.py\n",
    "\"\"\"\n",
    "Created on Wed Apr  6 10:32:20 2016\n",
    "\n",
    "@author: Junyue\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "This script accept a input sorted sam file, a output sam file, and a mismatch rate, then it will remove\n",
    "duplicates based on the barcode + UMI (edit distance <= 1), and chromatin and start site, at the same\n",
    "time, it will output the duplication number for each read, and generate the histogram plot for the read\n",
    "per duplication number\n",
    "'''\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def rm_dup_samfile(samfile, output_file, mismatch):\n",
    "    f1 = open(samfile)\n",
    "    f2 = open(output_file, 'w')\n",
    "    f3 = open(output_file+'.csv', 'w')\n",
    "    pre_barcode = set()\n",
    "    pre_chrom = 0\n",
    "    pre_site = 0\n",
    "    dup = False\n",
    "    \n",
    "    pre_dup_num = 0\n",
    "    cur_dup_num = 0\n",
    "    \n",
    "    for line in f1:\n",
    "        \n",
    "        if (line[0] == '@'):\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode_UMI = name[0] + name[1]\n",
    "            chrom_num = (line.split('\\t'))[2]\n",
    "            start_site = (line.split('\\t'))[3]\n",
    "            \n",
    "            if ((start_site == pre_site) and (chrom_num == pre_chrom)):\n",
    "                dup = False\n",
    "                for each_barcode in pre_barcode:\n",
    "                    edit_dis = distance(each_barcode, barcode_UMI)\n",
    "                    if edit_dis <= mismatch:\n",
    "                        dup = True\n",
    "                        break\n",
    "                \n",
    "                if dup == False:\n",
    "                    pre_dup_num = cur_dup_num\n",
    "                    cur_dup_num = 1\n",
    "                    f2.write(line)\n",
    "                    pre_barcode.add(barcode_UMI)\n",
    "                    f3.write('%d,%s' %(pre_dup_num, chrom_num))\n",
    "                    f3.write('\\n')\n",
    "                else:\n",
    "                    cur_dup_num += 1\n",
    "                    if edit_dis == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pre_barcode.add(barcode_UMI)\n",
    "    \n",
    "            else:\n",
    "                pre_dup_num = cur_dup_num\n",
    "                cur_dup_num = 1\n",
    "                f2.write(line)\n",
    "                pre_chrom = chrom_num\n",
    "                pre_site = start_site\n",
    "                pre_barcode = set()\n",
    "                pre_barcode.add(barcode_UMI)\n",
    "                if (pre_dup_num != 0):\n",
    "                    f3.write(\"%d\" % (pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    \n",
    "    #plot the histogram for the read duplication number\n",
    "    dups = (pd.read_csv(output_file+'.csv', header=None))[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(dups, bins=100)\n",
    "    plt.xlabel(\"Duplication number\")\n",
    "    plt.ylabel(\"Read number\")\n",
    "    fig.savefig(output_file + '.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samfile = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    mismatch = sys.argv[3]\n",
    "    rm_dup_samfile(samfile, output_file, mismatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Apr  6 10:32:20 2016\n",
    "\n",
    "@author: Junyue\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "This script accept a input sorted sam file, a output sam file, and a mismatch rate, then it will remove\n",
    "duplicates based on the barcode + UMI (edit distance <= 1), and chromatin and start site, at the same\n",
    "time, it will output the duplication number for each read, and generate the histogram plot for the read\n",
    "per duplication number\n",
    "'''\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def rm_dup_single_samfile(samfile, output_file, mismatch, dup_n):\n",
    "    f1 = open(samfile)\n",
    "    f2 = open(output_file, 'w')\n",
    "    \n",
    "    prebarcode=[]\n",
    "    pre_line = []\n",
    "    pre_dup = []\n",
    "    pre_chrom = 0\n",
    "    pre_site = 0\n",
    "    \n",
    "    # for each set of read with same chromatin start site, generate a list of barcode+UMI, and value with a\n",
    "    # list including the whole read, a list of duplication number\n",
    "    for line in f1:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sam_split_permuted.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sam_split_permuted.py\n",
    " \n",
    "'''\n",
    "in this script, I will read into the sam file and the barcode file, and then count the reads number per\n",
    "barcode; and at the same time, generate a permuted file for each output barcode\n",
    "\n",
    "'''\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def samfile_barcode_count(sam_file, barcode_file):\n",
    "    \n",
    "    #generate the barcode list and barcode dictionary\n",
    "    barcodes = open(barcode_file)\n",
    "    barcode_ls = []\n",
    "    barcode_dic = {}\n",
    "    for line in barcodes:\n",
    "        barcode = line.strip()\n",
    "        barcode_ls.append(barcode)\n",
    "        barcode_dic[barcode] = 0\n",
    "    barcodes.close()\n",
    "    #read the sam file, and count the number per barcode\n",
    "    sam = open(sam_file)\n",
    "    for line in sam:\n",
    "        if (line[0] == '@'):\n",
    "            continue\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode = name[0]\n",
    "            barcode_dic[barcode] += 1\n",
    "    sam.close()\n",
    "    return barcode_dic\n",
    "\n",
    "def permute_samples(sam_file, barcode_count, barcode_list, output_folder):\n",
    "    '''\n",
    "    this function accept a sam file, a barcode count dictionary, a barcode list and a output folder.\n",
    "    then for each barcode in the barcode list, it find the reads number n associated with the barcode, \n",
    "    and sample n reads from the samfile and then output the sampled reads to the \n",
    "    '''\n",
    "    \n",
    "    # Generate a list of output file\n",
    "    file_name = (sam_file.split('/')[-1]).split('.')[0]\n",
    "    output_files = {}\n",
    "    for barcode in barcode_list:\n",
    "        output_file = output_folder + '/' + file_name + '.' + barcode + '.permuted.sam'\n",
    "        output_files[barcode] = open(output_file, 'w')\n",
    "    \n",
    "    # output the header into each file and count the line number in the header and total reads number\n",
    "    header_number = 0\n",
    "    input_file = open(sam_file)\n",
    "    all_lines = input_file.readlines()\n",
    "    for line in all_lines:\n",
    "        if (line[0] == '@'):\n",
    "            header_number += 1\n",
    "            for barcode in barcode_list:\n",
    "                output_files[barcode].write(line)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # for each barcode, generate the permuted line array\n",
    "    # for each barcode, output the permuted lines to the output file\n",
    "    permuted_lines = np.random.permutation(all_lines[header_number:])\n",
    "    first_line = 0\n",
    "    for barcode in barcode_list:\n",
    "        end_line = first_line + barcode_count[barcode]\n",
    "        output_files[barcode].write(permuted_lines[first_line:end_line])\n",
    "        first_line = end_line\n",
    "    \n",
    "    # close the output file and sam file\n",
    "    for barcode in barcode_list:\n",
    "        output_files[barcode].close()\n",
    "    input_file.close()\n",
    "    \n",
    "def split_samfile(sam_file, barcode_file, output_folder, cutoff):\n",
    "    '''\n",
    "    this script accept a sam file, a barcode file, a output_file, a cutoff value,\n",
    "    then it will call the samfile_barcode_count function and get the total read count per barcode,\n",
    "    then it use the cutoff value to filter the barcode,\n",
    "    and generate the output samfile for single cells, generate the sample_ID.txt in the output folder,\n",
    "    generate the reads distribution in the output folder/read_distribution_barcode;\n",
    "    '''\n",
    "    \n",
    "    # generate the count per barcode\n",
    "    barcode_count = samfile_barcode_count(sam_file, barcode_file)\n",
    "    \n",
    "    # plot the barcode reads distribution and save the result to the ouput folder\n",
    "    plot_name = (sam_file.split('/')[-1]).split('.')[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(barcode_count.values(), bins=100)\n",
    "    plt.ylabel('frequency')\n",
    "    plt.xlabel('Number of unique reads')\n",
    "    fig_output = output_folder + '/' + plot_name + '.png'\n",
    "    \n",
    "    fig.savefig(fig_output)\n",
    "\n",
    "    #also output the barcode number and distribution to the output folder\n",
    "    read_dist = open(output_folder + '/' + plot_name + '.txt', 'w')\n",
    "    for barcode in barcode_count:\n",
    "        line = barcode + ', %d\\n' %(barcode_count[barcode])\n",
    "        read_dist.write(line)\n",
    "    read_dist.close()\n",
    "    \n",
    "    #filter the barcode based on the cutoff value\n",
    "    barcode_filtered = []\n",
    "    for barcode in barcode_count:\n",
    "        if barcode_count[barcode] >= cutoff:\n",
    "            barcode_filtered.append(barcode)\n",
    "    #print barcode_filtered\n",
    "     \n",
    "    # for the barcode in the barcode filter list, generate permuted sample in the output folder\n",
    "    print \"Generatign permuted sequences...\"\n",
    "    permute_samples(sam_file, barcode_list, barcode_filtered, output_folder)\n",
    "    \n",
    "    #generate the output sam file and sample_list file\n",
    "    sample_list_file = open(output_folder + '/' + plot_name + '.' + 'sample_list.txt', 'w')\n",
    "    output_files = {}\n",
    "    for barcode in barcode_filtered:\n",
    "        output_file = output_folder + '/' + plot_name + '.' + barcode + '.sam'\n",
    "        output_files[barcode] = open(output_file, 'w')\n",
    "        sample_list_file.write(plot_name + '.' + barcode + '\\n')\n",
    "    \n",
    "    # output the each read to the output sam file\n",
    "    sam = open(sam_file)\n",
    "    for line in sam:\n",
    "        if (line[0] == '@'):\n",
    "            for barcode in barcode_filtered:\n",
    "                output_files[barcode].write(line)\n",
    "        else:\n",
    "            barcode = (((line.split('\\t'))[0]).split(','))[0]\n",
    "            if barcode in barcode_filtered:\n",
    "                output_files[barcode].write(line)\n",
    "    \n",
    "    #close the files:\n",
    "    sample_list_file.close()\n",
    "    sam.close()\n",
    "    for barcode in barcode_filtered:\n",
    "        output_files[barcode].close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sam_file = sys.argv[1]\n",
    "    barcode_file = sys.argv[2]\n",
    "    output_folder = sys.argv[3]\n",
    "    cutoff = int(sys.argv[4])\n",
    "    split_samfile(sam_file, barcode_file, output_folder, cutoff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "in this script, I will read into the sam file and the barcode file, and then count the reads number per\n",
    "barcode; and at the same time, generate a permuted file for each output barcode\n",
    "\n",
    "'''\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def samfile_barcode_count(sam_file, barcode_file):\n",
    "    \n",
    "    #generate the barcode list and barcode dictionary\n",
    "    barcodes = open(barcode_file)\n",
    "    barcode_ls = []\n",
    "    barcode_dic = {}\n",
    "    for line in barcodes:\n",
    "        barcode = line.strip()\n",
    "        barcode_ls.append(barcode)\n",
    "        barcode_dic[barcode] = 0\n",
    "    barcodes.close()\n",
    "    #read the sam file, and count the number per barcode\n",
    "    sam = open(sam_file)\n",
    "    for line in sam:\n",
    "        if (line[0] == '@'):\n",
    "            continue\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode = name[0]\n",
    "            barcode_dic[barcode] += 1\n",
    "    sam.close()\n",
    "    return barcode_dic\n",
    "\n",
    "def permute_samples(sam_file, barcode_count, barcode_list, output_folder):\n",
    "    '''\n",
    "    this function accept a sam file, a barcode count dictionary, a barcode list and a output folder.\n",
    "    then for each barcode in the barcode list, it find the reads number n associated with the barcode, \n",
    "    and sample n reads from the samfile and then output the sampled reads to the \n",
    "    '''\n",
    "    \n",
    "    # Generate a list of output file\n",
    "    file_name = (sam_file.split('/')[-1]).split('.')[0]\n",
    "    output_files = {}\n",
    "    for barcode in barcode_list:\n",
    "        output_file = output_folder + '/' + file_name + '.' + barcode + '.permuted.sam'\n",
    "        output_files[barcode] = open(output_file, 'w')\n",
    "    \n",
    "    # output the header into each file and count the line number in the header and total reads number\n",
    "    header_number = 0\n",
    "    input_file = open(sam_file)\n",
    "    all_lines = input_file.readlines()\n",
    "    for line in all_lines:\n",
    "        if (line[0] == '@'):\n",
    "            header_number += 1\n",
    "            for barcode in barcode_list:\n",
    "                output_files[barcode].write(line)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # for each barcode, generate the permuted line array\n",
    "    # for each barcode, output the permuted lines to the output file\n",
    "    permuted_lines = np.random.permutation(all_lines[header_number:])\n",
    "    first_line = 0\n",
    "    for barcode in barcode_list:\n",
    "        end_line = first_line + barcode_count[barcode]\n",
    "        output_files[barcode].write(permuted_lines[first_line:end_line])\n",
    "        first_line = end_line\n",
    "    \n",
    "    # close the output file and sam file\n",
    "    for barcode in barcode_list:\n",
    "        output_files[barcode].close()\n",
    "    input_file.close()\n",
    "    \n",
    "def split_samfile(sam_file, barcode_file, output_folder, cutoff):\n",
    "    '''\n",
    "    this script accept a sam file, a barcode file, a output_file, a cutoff value,\n",
    "    then it will call the samfile_barcode_count function and get the total read count per barcode,\n",
    "    then it use the cutoff value to filter the barcode,\n",
    "    and generate the output samfile for single cells, generate the sample_ID.txt in the output folder,\n",
    "    generate the reads distribution in the output folder/read_distribution_barcode;\n",
    "    '''\n",
    "    \n",
    "    # generate the count per barcode\n",
    "    barcode_count = samfile_barcode_count(sam_file, barcode_file)\n",
    "    \n",
    "    # plot the barcode reads distribution and save the result to the ouput folder\n",
    "    plot_name = (sam_file.split('/')[-1]).split('.')[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(barcode_count.values(), bins=100)\n",
    "    plt.ylabel('frequency')\n",
    "    plt.xlabel('Number of unique reads')\n",
    "    fig_output = output_folder + '/' + plot_name + '.png'\n",
    "    \n",
    "    fig.savefig(fig_output)\n",
    "\n",
    "    #also output the barcode number and distribution to the output folder\n",
    "    read_dist = open(output_folder + '/' + plot_name + '.txt', 'w')\n",
    "    for barcode in barcode_count:\n",
    "        line = barcode + ', %d\\n' %(barcode_count[barcode])\n",
    "        read_dist.write(line)\n",
    "    read_dist.close()\n",
    "    \n",
    "    #filter the barcode based on the cutoff value\n",
    "    barcode_filtered = []\n",
    "    for barcode in barcode_count:\n",
    "        if barcode_count[barcode] >= cutoff:\n",
    "            barcode_filtered.append(barcode)\n",
    "    #print barcode_filtered\n",
    "     \n",
    "    # for the barcode in the barcode filter list, generate permuted sample in the output folder\n",
    "    print \"Generatign permuted sequences...\"\n",
    "    permute_samples(sam_file, barcode_list, barcode_filtered, output_folder)\n",
    "    \n",
    "    #generate the output sam file and sample_list file\n",
    "    sample_list_file = open(output_folder + '/' + plot_name + '.' + 'sample_list.txt', 'w')\n",
    "    output_files = {}\n",
    "    for barcode in barcode_filtered:\n",
    "        output_file = output_folder + '/' + plot_name + '.' + barcode + '.sam'\n",
    "        output_files[barcode] = open(output_file, 'w')\n",
    "        sample_list_file.write(plot_name + '.' + barcode + '\\n')\n",
    "    \n",
    "    # output the each read to the output sam file\n",
    "    sam = open(sam_file)\n",
    "    for line in sam:\n",
    "        if (line[0] == '@'):\n",
    "            for barcode in barcode_filtered:\n",
    "                output_files[barcode].write(line)\n",
    "        else:\n",
    "            barcode = (((line.split('\\t'))[0]).split(','))[0]\n",
    "            if barcode in barcode_filtered:\n",
    "                output_files[barcode].write(line)\n",
    "    \n",
    "    #close the files:\n",
    "    sample_list_file.close()\n",
    "    sam.close()\n",
    "    for barcode in barcode_filtered:\n",
    "        output_files[barcode].close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sam_file = sys.argv[1]\n",
    "    barcode_file = sys.argv[2]\n",
    "    output_folder = sys.argv[3]\n",
    "    cutoff = int(sys.argv[4])\n",
    "    split_samfile(sam_file, barcode_file, output_folder, cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a\\n', 'c\\n'], \n",
       "      dtype='|S2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a\\n', 'b\\n', 'c\\n', 'd\\n']\n",
    "a = np.random.permutation(a)\n",
    "a[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b\\n', 'd\\n'], \n",
       "      dtype='|S2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABC\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abc\\n'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here I transform the 384 barcode with lower case into upper case\n",
    "'''\n",
    "lowercase_384 = open('barcode_384_lowercase')\n",
    "uppercase_384 = open('barcode_384.txt', 'w')\n",
    "for line in lowercase_384:\n",
    "    uppercase_384.write(line.upper())\n",
    "lowercase_384.close()\n",
    "uppercase_384.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the recursion limit of the rm_dup script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./rm_dup_barcode_UMI.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./rm_dup_barcode_UMI.py\n",
    "\n",
    "'''\n",
    "This script accept a input sorted sam file, a output sam file, and a mismatch rate, then it will remove\n",
    "duplicates based on the barcode + UMI (edit distance <= 1), and chromatin and start site, at the same\n",
    "time, it will output the duplication number for each read, and generate the histogram plot for the read\n",
    "per duplication number\n",
    "'''\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "sys.setrecursionlimit(100000)\n",
    "def rm_dup_samfile(samfile, output_file, mismatch):\n",
    "    f1 = open(samfile)\n",
    "    f2 = open(output_file, 'w')\n",
    "    f3 = open(output_file+'.csv', 'w')\n",
    "    pre_barcode = []\n",
    "    pre_line = []\n",
    "    unique_id = []\n",
    "    pre_chrom = 0\n",
    "    pre_site = 0\n",
    "    dup = False\n",
    "    mismatch=int(mismatch)\n",
    "    \n",
    "    pre_dup_num = 0\n",
    "    cur_dup_num = 0\n",
    "    \n",
    "    for line in f1:\n",
    "        \n",
    "        if (line[0] == '@'):\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode_UMI = name[0] + name[1]\n",
    "            chrom_num = (line.split('\\t'))[2]\n",
    "            start_site = (line.split('\\t'))[3]\n",
    "            \n",
    "            if ((start_site == pre_site) and (chrom_num == pre_chrom)):\n",
    "                if (barcode_UMI in pre_barcode):\n",
    "                    cur_dup_num += 1\n",
    "                else:\n",
    "                    pre_barcode.append(barcode_UMI)\n",
    "                    pre_line.append(line)\n",
    "                '''\n",
    "                dup = False\n",
    "                for each_barcode in pre_barcode:\n",
    "                    edit_dis = distance(each_barcode, barcode_UMI)\n",
    "                    if edit_dis <= mismatch:\n",
    "                        dup = True\n",
    "                        break\n",
    "                \n",
    "                if dup == False:\n",
    "                    pre_dup_num = cur_dup_num\n",
    "                    cur_dup_num = 1\n",
    "                    f2.write(line)\n",
    "                    pre_barcode.add(barcode_UMI)\n",
    "                    f3.write('%d' %(pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "                else:\n",
    "                    cur_dup_num += 1\n",
    "                    if edit_dis == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pre_barcode.add(barcode_UMI)\n",
    "    '''\n",
    "            else:\n",
    "                if (pre_barcode != []):\n",
    "                    unique_id = index_unique(pre_barcode, mismatch)\n",
    "                    for i in unique_id:\n",
    "                        f2.write(pre_line[i])\n",
    "                    cur_dup_num = cur_dup_num + len(pre_barcode) - len(unique_id)\n",
    "                \n",
    "                pre_dup_num = cur_dup_num\n",
    "                cur_dup_num = 1\n",
    "                unique_id = []\n",
    "                pre_barcode = []\n",
    "                pre_line = []\n",
    "                pre_chrom = chrom_num\n",
    "                pre_site = start_site\n",
    "                \n",
    "                pre_barcode.append(barcode_UMI)\n",
    "                pre_line.append(line)\n",
    "                if (pre_dup_num != 0):\n",
    "                    f3.write(\"%d\" % (pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "    \n",
    "    # also count for the last reads\n",
    "    if (pre_barcode != []):\n",
    "        unique_id = index_unique(pre_barcode, mismatch)\n",
    "        for i in unique_id:\n",
    "            f2.write(pre_line[i])\n",
    "        cur_dup_num = cur_dup_num + len(pre_barcode) - len(unique_id)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    \n",
    "    '''\n",
    "    #plot the histogram for the read duplication number\n",
    "    dups = (pd.read_csv(output_file+'.csv', header=None))[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(dups, bins=100)\n",
    "    plt.xlabel(\"Duplication number\")\n",
    "    plt.ylabel(\"Read number\")\n",
    "    fig.savefig(output_file + '.png')\n",
    "    '''\n",
    "\n",
    "def index_unique(barcodes_list, cutoff_value):\n",
    "    # first I am going to create a matrix that include all the pairs of barcodes\n",
    "    list_length = len(barcodes_list)\n",
    "    distance_matrix = np.arange(list_length * list_length).reshape(list_length, list_length)\n",
    "    \n",
    "    for i in range(list_length):\n",
    "        for j in range(list_length):\n",
    "            if (j < i):\n",
    "                distance_matrix[i][j] = distance_matrix[j][i]\n",
    "            elif (j == i):\n",
    "                distance_matrix[i][j] = 0\n",
    "            else:\n",
    "                distance_matrix[i][j] = distance(barcodes_list[i], barcodes_list[j])\n",
    "    #print distance_matrix\n",
    "    # Then I am going to create a list of indexs\n",
    "    unique_index = []\n",
    "    non_visited_indexes = range(list_length)\n",
    "    # for each element in the index list, I am going remove its duplicates based on the cutoff_value\n",
    "    for i in range(list_length):\n",
    "        if i in non_visited_indexes:\n",
    "            unique_index.append(i)\n",
    "            rm_dup(non_visited_indexes, distance_matrix, i, cutoff_value)\n",
    "    \n",
    "    return unique_index\n",
    "\n",
    "def rm_dup(indexes, distance_matrix, n, cutoff_value):\n",
    "    # this script accept a index list, a distance matrix, and a index n; and then it remove the duplicates of index value n\n",
    "    # and cutoff_value\n",
    "    if n in indexes:\n",
    "        indexes.remove(n)\n",
    "    \n",
    "    all_indexes = indexes[:]\n",
    "    for i in range(len(all_indexes)):\n",
    "        #print all_indexes, indexes, i, n, all_indexes[i]\n",
    "        if all_indexes[i] in indexes:\n",
    "            if (distance_matrix[all_indexes[i], n] <= cutoff_value):\n",
    "                #print i, n, distance_matrix[all_indexes[i], n], indexes\n",
    "                indexes = rm_dup(indexes, distance_matrix, all_indexes[i], cutoff_value)\n",
    "    return indexes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samfile = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    mismatch = sys.argv[3]\n",
    "    rm_dup_samfile(samfile, output_file, mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./gene_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./gene_count.py\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import *\n",
    "import HTSeq\n",
    "import sys\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# read in the sam file and then count gene body coverage along transcripts\n",
    "\n",
    "def sciRNA_gene_body_transcripts(gtf_file, input_file, output_file):\n",
    "    # read in the gtf file, and then construct the genome interval for exons, genes, and gene end dictionary\n",
    "    gtf_file = HTSeq.GFF_Reader(gtf_file, end_included=True)\n",
    "    gene_coverage_file = input_folder + \"/gene_coverage.txt\"\n",
    "    gene_coverage = open(gene_coverage_file, \"w\")\n",
    "    transcripts = HTSeq.GenomicArrayOfSets( \"auto\", stranded=True )\n",
    "    transcript_n = 0\n",
    "    dic_interval = {}\n",
    "    \n",
    "    print(\"Start calculating transcript genomic arrays....\")\n",
    "\n",
    "    for feature in gtf_file:\n",
    "\n",
    "        if feature.type == \"transcript\":\n",
    "            transcript_n += 1\n",
    "            transcripts[ feature.iv ] += feature.attr[\"transcript_id\"]\n",
    "            dic_interval[feature.attr[\"transcript_id\"]] = feature.iv\n",
    "            #print \"feature gene name: \", feature.attr[\"gene_id\"]\n",
    "            \n",
    "    print(\"Detected transcript number: \", transcript_n)\n",
    "    \n",
    "    gene_annotat.close()\n",
    "    \n",
    "    # count the gene body coverage for the sample\n",
    "    sam_file = input_file\n",
    "    almnt_file = HTSeq.SAM_Reader(sam_file)\n",
    "    gene_cover = []\n",
    "    for alnmt in almnt_file:\n",
    "        #print alnmt\n",
    "        if not alnmt.aligned:\n",
    "            continue\n",
    "        if alnmt.iv.chrom not in genes.chrom_vectors:\n",
    "            continue\n",
    "\n",
    "        gene_id_intersect = set()\n",
    "        inter_count = 0\n",
    "        for cigop in alnmt.cigar:\n",
    "            if cigop.type != \"M\":\n",
    "                continue\n",
    "            for iv,val in transcripts[cigop.ref_iv].steps():\n",
    "                #print iv, val\n",
    "                if inter_count == 0:\n",
    "                    gene_id_intersect |= val\n",
    "                    inter_count += 1\n",
    "                else:\n",
    "                    gene_id_intersect &= val\n",
    "            \n",
    "            if len(gene_id_intersect) == 1:\n",
    "                gene_id = list(gene_id_combine)[0]\n",
    "                percent = compute_percent(alnmt.iv, dic_interval[gene_id])\n",
    "                gene_cover.append(percent)\n",
    "        \n",
    "    gene_count = pd.DataFrame({\"gene_coverage\":gene_cover})\n",
    "    gene_count.to_csv(output_file)\n",
    "    print(\"All analysis done~\")\n",
    "\n",
    "def compute_percent(aln_iv, ts_iv):\n",
    "    '''\n",
    "    In this function, I am going to accept a interval from sam file read, and a interval from a transcripts, and then\n",
    "    compute the distance between the end of the alignment interval and the start of the transcripts, and divide the\n",
    "    distance with the distance of the transcript interval\n",
    "    '''\n",
    "    percent = abs(aln_iv.end_d - ts_iv.stard_d) / ts_iv.length\n",
    "    return percent\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    gtf_file = sys.argv[1]\n",
    "    input_file = sys.argv[2]\n",
    "    output_file = sys.argv[3]\n",
    "    sciRNA_gene_body_transcripts(gtf_file, input_file, output_file)\n",
    "    \n",
    "     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import *\n",
    "import HTSeq\n",
    "import sys\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# read in the sam file and then count gene body coverage along transcripts\n",
    "\n",
    "def sciRNA_gene_body_transcripts(gtf_file, input_file, output_file):\n",
    "    # read in the gtf file, and then construct the genome interval for exons, genes, and gene end dictionary\n",
    "    gtf_file = HTSeq.GFF_Reader(gtf_file, end_included=True)\n",
    "    gene_coverage_file = input_folder + \"/gene_coverage.txt\"\n",
    "    gene_coverage = open(gene_coverage_file, \"w\")\n",
    "    transcripts = HTSeq.GenomicArrayOfSets( \"auto\", stranded=True )\n",
    "    transcript_n = 0\n",
    "    dic_interval = {}\n",
    "    \n",
    "    print(\"Start calculating transcript genomic arrays....\")\n",
    "\n",
    "    for feature in gtf_file:\n",
    "\n",
    "        if feature.type == \"transcript\":\n",
    "            transcript_n += 1\n",
    "            transcripts[ feature.iv ] += feature.attr[\"transcript_id\"]\n",
    "            dic_interval[feature.attr[\"transcript_id\"]] = feature.iv\n",
    "            #print \"feature gene name: \", feature.attr[\"gene_id\"]\n",
    "            \n",
    "    print(\"Detected transcript number: \", transcript_n)\n",
    "    \n",
    "    gene_annotat.close()\n",
    "    \n",
    "    # count the gene body coverage for the sample\n",
    "    sam_file = input_file\n",
    "    almnt_file = HTSeq.SAM_Reader(sam_file)\n",
    "    gene_cover = []\n",
    "    for alnmt in almnt_file:\n",
    "        #print alnmt\n",
    "        if not alnmt.aligned:\n",
    "            continue\n",
    "        if alnmt.iv.chrom not in genes.chrom_vectors:\n",
    "            continue\n",
    "\n",
    "        gene_id_intersect = set()\n",
    "        inter_count = 0\n",
    "        for cigop in alnmt.cigar:\n",
    "            if cigop.type != \"M\":\n",
    "                continue\n",
    "            for iv,val in transcripts[cigop.ref_iv].steps():\n",
    "                #print iv, val\n",
    "                if inter_count == 0:\n",
    "                    gene_id_intersect |= val\n",
    "                    inter_count += 1\n",
    "                else:\n",
    "                    gene_id_intersect &= val\n",
    "            \n",
    "            if len(gene_id_intersect) == 1:\n",
    "                gene_id = list(gene_id_combine)[0]\n",
    "                percent = compute_percent(alnmt.iv, dic_interval[gene_id])\n",
    "                gene_cover.append(percent)\n",
    "        \n",
    "    gene_count = pd.DataFrame({\"gene_coverage\":gene_cover})\n",
    "    gene_count.to_csv(output_file)\n",
    "    print(\"All analysis done~\")\n",
    "\n",
    "def compute_percent(aln_iv, ts_iv):\n",
    "    '''\n",
    "    In this function, I am going to accept a interval from sam file read, and a interval from a transcripts, and then\n",
    "    compute the distance between the end of the alignment interval and the start of the transcripts, and divide the\n",
    "    distance with the distance of the transcript interval\n",
    "    '''\n",
    "    percent = abs(aln_iv.end_d - ts_iv.stard_d) / ts_iv.length\n",
    "    return percent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame({\"a\":[1, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### In this script, I am going to generate the script for sequence analysis of 3 level indexing with indexed Tn5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ../ATAC_RNA_coassay_pipe/scRNA_seq/* ./index_Tn5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./index_Tn5/RNA_split_fastq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./index_Tn5/RNA_split_fastq.py\n",
    "# python RNA_split_fastq.py read1 read2 output_folder P5_index P5_P7_index\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import gzip\n",
    "from Levenshtein import distance\n",
    "\n",
    "'''\n",
    "This script accept a read1, read2 file of RNA-seq, a barcode file P5 file, a corresponding P7 P5 barcode file in \n",
    "ATAC-seq, output file folder, then it read in the read1 and read2 file, split the read1 and read2 based on\n",
    "the combination of the P5 barcode, and attach the corresponding P7.P5 index to the read1 and read2 sequence\n",
    "For each read1, I will go through line by line, for line with the index line,\n",
    "extract the index, convert the index, and file which this index belong to, and then write the read1 and read2 both into the \n",
    "output file.\n",
    "'''\n",
    "\n",
    "def extract_barcode(index):\n",
    "    '''\n",
    "    This function accept a index sequence, and return the extracted P7+P5 barcode\n",
    "    in the index, and the barcode in the Tn5 adaptor\n",
    "    '''\n",
    "    barcodes = index.split('+')\n",
    "    P_index = barcodes[1]\n",
    "    N_index = barcodes[0][0:8]\n",
    "    \n",
    "    return (P_index, N_index)\n",
    "\n",
    "def all_indexes(P_index, P5_P7_index):\n",
    "    '''\n",
    "    this function accept a text including P7 index and a text file including a \n",
    "    P5 index and then generate a list for all the P7.P5 barcode combinations\n",
    "    '''\n",
    "    index = open(P_index)\n",
    "    P5_P7 = open(P5_P7_index)\n",
    "    index1 = index.readlines()\n",
    "    index2 = P5_P7.readlines()\n",
    "    index1 = map(lambda x: x.strip(), index1)\n",
    "    index2 = map(lambda x: x.strip(), index2)\n",
    "    all_index = {}\n",
    "    for i in range(len(index1)):\n",
    "            all_index[index1[i]] = index2[i]\n",
    "    \n",
    "    index.close()\n",
    "    P5_P7.close()\n",
    "    return all_index\n",
    "\n",
    "def find_P7_P5_barcode(index, all_index):\n",
    "    '''\n",
    "    this function accept a P7_P5 barcode in the read, and a list of P7_P5 barcodes provided,\n",
    "    and then find the barcode in the P7_P5 barcode list that match the index with distance <2,\n",
    "    then return the found barcode; if no barcode is found, then return -1\n",
    "    '''\n",
    "    result = -1\n",
    "    P5_barcodes = all_index.keys()\n",
    "\n",
    "    for barcode in P5_barcodes:\n",
    "        diff = distance(index, barcode)\n",
    "        if (diff <= 1):\n",
    "            result = all_index[barcode]\n",
    "            break\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def split_fastq(read1, read2, output_folder, all_index): \n",
    "    '''\n",
    "    This function accept a read1 file, a read2 file, a dictionary from P5 oligo-dT barcode to ATAC-seq P5 P7 barcode, \n",
    "    create the output files,\n",
    "    and then go through each line of the read1 and read2 file,\n",
    "    and then check the barcode in the read1, compare it with the P7_P5 barcodes,\n",
    "    if the distance < 2, then output the reads into the output files\n",
    "    '''\n",
    "    \n",
    "    # generate the output files\n",
    "    output_read1 = {}\n",
    "    output_read2 = {}\n",
    "    sample_ID = open(output_folder + \"/../sample_ID.txt\", 'w')\n",
    "    for barcode in all_index.values():\n",
    "        output_R1 = output_folder + '/' + barcode + '.R1.fastq'\n",
    "        output_R2 = output_folder + '/' + barcode + '.R2.fastq'\n",
    "        output_read1[barcode] = open(output_R1, 'w')\n",
    "        output_read2[barcode] = open(output_R2, 'w')\n",
    "        sample_ID.write(barcode + '\\n')\n",
    "    sample_ID.close()\n",
    "    \n",
    "    # open the read1 and read2 file, and then for each read, check the barcode and output the matched read into the\n",
    "    # output file\n",
    "    \n",
    "    f1 = gzip.open(read1)\n",
    "    f2 = gzip.open(read2)\n",
    "    line_n = 0\n",
    "    \n",
    "    while(True):\n",
    "        line1 = f1.readline()\n",
    "        line2 = f2.readline()\n",
    "        line_n += 1\n",
    "        if (not line1):\n",
    "            break\n",
    "        if (line_n % 4 == 1):\n",
    "            index = line1.strip().split(':')[-1]\n",
    "            index = extract_barcode(index)\n",
    "            P_index = index[0]\n",
    "            N_index = index[1]\n",
    "            P_index = find_P7_P5_barcode(P_index, all_index)\n",
    "            if (P_index != -1):\n",
    "                output_R1 = output_read1[P_index]\n",
    "                output_R2 = output_read2[P_index]\n",
    "                line1 = '@' + N_index + ',' + line1[1:]\n",
    "                line2 = '@' + N_index + ',' + line2[1:]\n",
    "                output_R1.write(line1)\n",
    "                output_R2.write(line2)\n",
    "                \n",
    "                # output the other lines into the files\n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                output_R1.write(line1)\n",
    "                output_R2.write(line2)\n",
    "                line_n += 1\n",
    "                \n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                output_R1.write(line1)\n",
    "                output_R2.write(line2)\n",
    "                line_n += 1\n",
    "                \n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                output_R1.write(line1)\n",
    "                output_R2.write(line2)\n",
    "                line_n += 1\n",
    "                \n",
    "            else:\n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                line_n += 1\n",
    "                \n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                line_n += 1\n",
    "                \n",
    "                line1 = f1.readline()\n",
    "                line2 = f2.readline()\n",
    "                line_n += 1\n",
    "    # close the files\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    for barcode in all_index.values():\n",
    "        output_read1[barcode].close()\n",
    "        output_read2[barcode].close()\n",
    "\n",
    "def split_fastq_main(read1, read2, output_folder, P5_file, P7_P5_file):\n",
    "    '''\n",
    "    this is the main function that accept the read1 file, the read2 file, the output folder,\n",
    "    the files including P7 barcode, P5 barcode and then split the \n",
    "    the read1 and read2 files based on the P5 and P7 combinations\n",
    "    '''\n",
    "    print \"Start splitting the fastq files...\"\n",
    "    all_index = all_indexes(P5_file, P7_P5_file)\n",
    "    print \"Number of output files: \", len(all_index)\n",
    "    split_fastq(read1, read2, output_folder, all_index)\n",
    "    print \"fastq files splitted~\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    read1 = sys.argv[1]\n",
    "    read2 = sys.argv[2]\n",
    "    output_folder = sys.argv[3]\n",
    "    P5_file = sys.argv[4]\n",
    "    P7_P5_file = sys.argv[5]\n",
    "    split_fastq_main(read1, read2, output_folder, P5_file, P7_P5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./index_Tn5/RNA_split_fastq.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./index_Tn5/RNA_split_fastq.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# this script take a read1 file, a read2 file, a output folder, a P5 index (oligo-dT file), a P7_P5 index file, and then split the RNA-seq data\n",
    "# in the coassay based on the oligo-dT P5 index and transform it to the corresponding ATAC-seq index, and output the splitted files into the \n",
    "# output folder\n",
    "\n",
    "read1=$1\n",
    "read2=$2\n",
    "output_folder=$3\n",
    "P5_index=$4\n",
    "P7_P5_index=$P5_index\n",
    "\n",
    "script=\"/net/shendure/vol1/home/cao1025/analysis_script/scRNA_seq_pipe/index_Tn5/RNA_split_fastq.py\"\n",
    "echo \"Start split the RNA-seq data...\"\n",
    "mkdir -p $output_folder\n",
    "python $script $read1 $read2 $output_folder $P5_index $P7_P5_index\n",
    "echo \"Fastq file splitted~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./sample_read_n_fastq.py\n",
    "# First, I am going to write a script, that accept fastq read2 file (UMI attached), and then count the read number\n",
    "# for each barcode, and filter out the barcode with lower reads compared with the threshold and then sample the number\n",
    "# equal to the threshold for the rest of the barcodes\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import HTSeq\n",
    "from Levenshtein import distance\n",
    "\n",
    "# In this script, it will read a barcode file and then return a list of barcodes\n",
    "def read_barcode(barcode_file):\n",
    "    read_barcodes = open(barcode_file, 'r')\n",
    "    barcodes = read_barcodes.readlines()\n",
    "    for i in range(len(barcodes)):\n",
    "        barcodes[i] = barcodes[i].strip().upper()\n",
    "    read_barcodes.close()\n",
    "    return barcodes\n",
    "\n",
    "# In this script, I will read in a read1 file, a barcode list and then generate a dictionary\n",
    "# has barcode and UMI count (barcode corrected)\n",
    "def analyze_read(read_file, barcode_ls):\n",
    "    # read through the read1 file, and calculate the UMI number of each barcode (corrected \n",
    "    # edit distance of 1)\n",
    "    read1 = HTSeq.FastqReader(read_file)\n",
    "    \n",
    "    # initiate the dictionary\n",
    "    result = {}\n",
    "    for barcode in barcode_ls:\n",
    "        result[barcode] = 0\n",
    "    \n",
    "    for line in read1:\n",
    "        # extract the barcode line and compare with each barcode\n",
    "        barcode = line.name[0:10]\n",
    "        result[barcode] += 1\n",
    "    return result\n",
    "\n",
    "def sample_reads(barcode_ls, barcode_read_n, read_file, output_file, limit_n):\n",
    "    # This function accept a barcode file, a read file, a output file and a limit n and then\n",
    "    # (1) generate a list of possible barcode\n",
    "    # (2) count the number of barcode in the read file\n",
    "    # (3) filter barcode based on read number > limit_n\n",
    "    # (4) for the filtered barcode, output the number of barcode (limit_n) into the output file\n",
    "    \n",
    "    #barcode_ls = read_barcode(barcode_file)\n",
    "    #barcode_read_n = analyze_read(read_file, barcode_ls)\n",
    "    limit_n = int(limit_n)\n",
    "    barcode_dic = {}\n",
    "    for barcode in barcode_read_n:\n",
    "        if barcode_read_n[barcode] >= limit_n:\n",
    "            barcode_dic[barcode] = 0\n",
    "    reads = HTSeq.FastqReader(read_file)\n",
    "    \n",
    "    output = open(output_file, 'w')\n",
    "    for read in reads:\n",
    "        # if the barcode in the read is in the filtered barcode, and the read ouput number of this barcode is still < \n",
    "        # limit_n, then output the read into the output file\n",
    "        barcode = read.name[0:10]\n",
    "        if barcode in barcode_dic:\n",
    "            if barcode_dic[barcode] < limit_n:\n",
    "                read.write_to_fastq_file(output)\n",
    "                barcode_dic[barcode] += 1\n",
    "    output.close()\n",
    "\n",
    "def sample_reads_file(barcode_file, read_file, output_file, limit_n):\n",
    "    barcode_ls = read_barcode(barcode_file)\n",
    "    print \"Barcode list is generated...\"\n",
    "    print \"Start calculating the reads number per barcode...\"\n",
    "    barcode_read_n = analyze_read(read_file, barcode_ls)\n",
    "    print \"Read number calculation per barcode is calculated.\"\n",
    "    print \"Start writing to output files...\"\n",
    "    sample_reads(barcode_ls, barcode_read_n, read_file, output_file, limit_n)\n",
    "    print \"Output files written is done.  \"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    barcode_file = sys.argv[1]\n",
    "    read_file = sys.argv[2]\n",
    "    output_file = sys.argv[3]\n",
    "    limit_n = sys.argv[4]\n",
    "    sample_reads_file(barcode_file, read_file, output_file, limit_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./index_Tn5/UMI_attach_barcode_UMI.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# this script take in a input folder, a sample ID, a output folder, a oligo-dT barcode file, a corresponding N5 barcode file, and\n",
    "# it pass the factors to the python script\n",
    "\n",
    "input_folder=$1\n",
    "sample_ID=$2\n",
    "output_folder=$3\n",
    "oligo_dT_barcode=$4\n",
    "N5_barcode=$5\n",
    "script_folder=~/analysis_script/ATAC_RNA_coassay_pipe/scRNA_seq\n",
    "\n",
    "echo \"Transform the cell barcode in Read1 to N5 barcode and attach to read2 with UMI....\"\n",
    "\n",
    "python $script_folder/UMI_attach_barcode_UMI.py $input_folder $sample_ID $output_folder $oligo_dT_barcode $N5_barcode\n",
    "\n",
    "echo \"Barcode transformed to N5 barcode and attached to read2 with UMI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./index_Tn5/UMI_attach_barcode_UMI.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./index_Tn5/UMI_attach_barcode_UMI.py\n",
    "\"\"\"\n",
    "Created on Tue Apr  5 22:16:54 2016\n",
    "\n",
    "@author: Junyue\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from Levenshtein import distance\n",
    "import gzip\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "'''\n",
    "    this script accept a read1 file, a read2 file, a output_file, a oligodT, barcode list,a N5 barcode list\n",
    "    and mismatch rate, then it open the read1 and read2, output file,\n",
    "    then extract the barcode and UMI sequence in the read 1 file, and convert the\n",
    "    barcode to the real barcode in the barcode list based on the mismatch rate,\n",
    "    and convert the barcode to the N5 barcode\n",
    "    then it attach the barcode and UMI sequence to the read name of the read2 file\n",
    "'''\n",
    "\n",
    "def UMI_attach_read2_barcode_list(sample, input_folder, output_folder, barcode_list, barcode_N7, mismatch_rate = 1):\n",
    "    #open the read1, read2, and output file\n",
    "    Read1 = input_folder + \"/\" + sample + \".R1.fastq.gz\"\n",
    "    Read2 = input_folder + \"/\" + sample + \".R2.fastq.gz\"\n",
    "    output_file = output_folder + \"/\" + sample + \".R2.fastq.gz\"\n",
    "    mismatch_rate = int(mismatch_rate)\n",
    "    f1 = gzip.open(Read1)\n",
    "    f2 = gzip.open(Read2)\n",
    "    f3 = gzip.open(output_file, 'wb')\n",
    "    \n",
    "    line1 = f1.readline()\n",
    "    line2 = f2.readline()\n",
    "\n",
    "    while (line1):\n",
    "        line1 = f1.readline()\n",
    "        target = line1[8:18]\n",
    "        \n",
    "        for barcode in barcode_list:\n",
    "            mismatch = distance(barcode, target)\n",
    "            find = False\n",
    "            \n",
    "            if (mismatch <= mismatch_rate):\n",
    "                UMI = line1[:8]\n",
    "                bc_N7 = line2[1:9]\n",
    "                for bc in barcode_N7:\n",
    "                    mismatch = distance(bc_N7, bc)\n",
    "                    find = False\n",
    "                    if (mismatch <= mismatch_rate):\n",
    "                        find = True\n",
    "                        first_line = '@' + bc + '.' + barcode + ',' + UMI + ',' + line2[9:]\n",
    "                        f3.write(first_line)\n",
    "\n",
    "                        second_line = f2.readline()\n",
    "                        f3.write(second_line)\n",
    "\n",
    "                        third_line = f2.readline()\n",
    "                        f3.write(third_line)\n",
    "\n",
    "                        four_line = f2.readline()\n",
    "                        f3.write(four_line)\n",
    "\n",
    "                        line2 = f2.readline()\n",
    "                        break\n",
    "                if find == True:\n",
    "                    break\n",
    "                \n",
    "        if find == False:\n",
    "            line2 = f2.readline()\n",
    "            line2 = f2.readline()\n",
    "            line2 = f2.readline()\n",
    "            line2 = f2.readline()\n",
    "\n",
    "        line1 = f1.readline()\n",
    "        line1 = f1.readline()\n",
    "        line1 = f1.readline()\n",
    "\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "\n",
    "# this function accept an input folder and a output folder and then generate the output file with the index\n",
    "def attach_UMI_files(input_folder, sampleID, output_folder, barcode_file, N7_barcode_file, core_number):\n",
    "    \n",
    "    init_message = '''\n",
    "    --------------------------start attaching UMI-----------------------------\n",
    "    input folder: %s\n",
    "    sample ID: %s\n",
    "    output_folder: %s\n",
    "    barcode file: %s\n",
    "    N7 barcode file: %s\n",
    "    ___________________________________________________________________________\n",
    "    ''' %(input_folder, sampleID, output_folder, barcode_file, N7_barcode_file)\n",
    "    \n",
    "    print(init_message)\n",
    "    \n",
    "    # generate the barcode list:\n",
    "    barcode_list = []\n",
    "    barcodes = open(barcode_file)\n",
    "    for barcode in barcodes:\n",
    "        barcode_list.append(barcode.strip())\n",
    "    barcodes.close()\n",
    "    \n",
    "    # input the N7 barcode list\n",
    "    N7_barcodes = open(N7_barcode_file)\n",
    "    barcode_N7 = []\n",
    "    for barcode in N7_barcodes:\n",
    "        barcode_N7.append(barcode.strip())\n",
    "    \n",
    "    #for each sample in the sample list, use the read1 file, read2 file, output file\n",
    "    # and barcode_list to run UMI_attach_read2_barcode_list\n",
    "    sample_file = open(sampleID)\n",
    "    sample_list = []\n",
    "    for line in sample_file:\n",
    "        sample = line.strip()\n",
    "        sample_list.append(sample)\n",
    "    sample_file.close()\n",
    "    \n",
    "    # parallele for the functions\n",
    "    p = Pool(processes = int(core_number))\n",
    "    #print(\"Processing core number: \", core_number)\n",
    "    func = partial(UMI_attach_read2_barcode_list, input_folder = input_folder, output_folder=output_folder, barcode_list=barcode_list, barcode_N7 = barcode_N7, mismatch_rate = 1)\n",
    "    #sciRNAseq_count(sample, input_folder, exons, genes, gene_end)\n",
    "    result = p.map(func, sample_list)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    #print the completion message\n",
    "    com_message = '''~~~~~~~~~~~~~~~UMI attachment done~~~~~~~~~~~~~~~~~~'''\n",
    "    print com_message\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = sys.argv[1]\n",
    "    sampleID = sys.argv[2]\n",
    "    output_folder = sys.argv[3]\n",
    "    barcode_file = sys.argv[4]\n",
    "    N7_barcode_file = sys.argv[5]\n",
    "    core=sys.argv[6]\n",
    "    attach_UMI_files(input_folder, sampleID, output_folder, barcode_file, N7_barcode_file, core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./index_Tn5/UMI_attach_barcode_UMI.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./index_Tn5/UMI_attach_barcode_UMI.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# this script take in a input folder, a sample ID, a output folder, a oligo-dT barcode file, a corresponding N5 barcode file, and\n",
    "# it pass the factors to the python script\n",
    "\n",
    "input_folder=$1\n",
    "sample_ID=$2\n",
    "output_folder=$3\n",
    "oligo_dT_barcode=$4\n",
    "N7_barcode=$5\n",
    "core=$6\n",
    "script=\"/net/shendure/vol1/home/cao1025/analysis_script/scRNA_seq_pipe/index_Tn5/UMI_attach_barcode_UMI.py\"\n",
    "\n",
    "echo \"Attaching barcode and UMI....\"\n",
    "\n",
    "mkdir -p $output_folder\n",
    "python $script $input_folder $sample_ID $output_folder $oligo_dT_barcode $N7_barcode $core\n",
    "\n",
    "echo \"Barcode transformed to N5 barcode and attached to read2 with UMI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./index_Tn5/rm_dup_barcode_UMI.py\n",
    "\n",
    "'''\n",
    "This script accept a input sorted sam file, a output sam file, and a mismatch rate, then it will remove\n",
    "duplicates based on the barcode + UMI (edit distance <= 1), and chromatin and start site, at the same\n",
    "time, it will output the duplication number for each read, and generate the histogram plot for the read\n",
    "per duplication number\n",
    "'''\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def rm_dup_samfile(samfile, output_file, mismatch):\n",
    "    f1 = open(samfile)\n",
    "    f2 = open(output_file, 'w')\n",
    "    f3 = open(output_file+'.csv', 'w')\n",
    "    pre_barcode = []\n",
    "    pre_line = []\n",
    "    unique_id = []\n",
    "    pre_chrom = 0\n",
    "    pre_site = 0\n",
    "    dup = False\n",
    "    \n",
    "    pre_dup_num = 0\n",
    "    cur_dup_num = 0\n",
    "    mismatch = int(mismatch)\n",
    "    for line in f1:\n",
    "        \n",
    "        if (line[0] == '@'):\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode_UMI = name[0] + name[1]\n",
    "            chrom_num = (line.split('\\t'))[2]\n",
    "            start_site = (line.split('\\t'))[3]\n",
    "            \n",
    "            if ((start_site == pre_site) and (chrom_num == pre_chrom)):\n",
    "                if (barcode_UMI in pre_barcode):\n",
    "                    cur_dup_num += 1\n",
    "                else:\n",
    "                    pre_barcode.append(barcode_UMI)\n",
    "                    pre_line.append(line)\n",
    "                '''\n",
    "                dup = False\n",
    "                for each_barcode in pre_barcode:\n",
    "                    edit_dis = distance(each_barcode, barcode_UMI)\n",
    "                    if edit_dis <= mismatch:\n",
    "                        dup = True\n",
    "                        break\n",
    "                \n",
    "                if dup == False:\n",
    "                    pre_dup_num = cur_dup_num\n",
    "                    cur_dup_num = 1\n",
    "                    f2.write(line)\n",
    "                    pre_barcode.add(barcode_UMI)\n",
    "                    f3.write('%d' %(pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "                else:\n",
    "                    cur_dup_num += 1\n",
    "                    if edit_dis == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pre_barcode.add(barcode_UMI)\n",
    "    '''\n",
    "            else:\n",
    "                if (pre_barcode != []):\n",
    "                    unique_id = index_unique(pre_barcode, mismatch)\n",
    "                    for i in unique_id:\n",
    "                        f2.write(pre_line[i])\n",
    "                    cur_dup_num = cur_dup_num + len(pre_barcode) - len(unique_id)\n",
    "                \n",
    "                pre_dup_num = cur_dup_num\n",
    "                cur_dup_num = 1\n",
    "                unique_id = []\n",
    "                pre_barcode = []\n",
    "                pre_line = []\n",
    "                pre_chrom = chrom_num\n",
    "                pre_site = start_site\n",
    "                \n",
    "                pre_barcode.append(barcode_UMI)\n",
    "                pre_line.append(line)\n",
    "                if (pre_dup_num != 0):\n",
    "                    f3.write(\"%d\" % (pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    \n",
    "    #plot the histogram for the read duplication number\n",
    "    dups = (pd.read_csv(output_file+'.csv', header=None))[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(dups, bins=100)\n",
    "    plt.xlabel(\"Duplication number\")\n",
    "    plt.ylabel(\"Read number\")\n",
    "    fig.savefig(output_file + '.png')\n",
    "\n",
    "def index_unique(barcodes_list, cutoff_value):\n",
    "    # first I am going to create a matrix that include all the pairs of barcodes\n",
    "    list_length = len(barcodes_list)\n",
    "    distance_matrix = np.arange(list_length * list_length).reshape(list_length, list_length)\n",
    "    \n",
    "    for i in range(list_length):\n",
    "        for j in range(list_length):\n",
    "            if (j < i):\n",
    "                distance_matrix[i][j] = distance_matrix[j][i]\n",
    "            elif (j == i):\n",
    "                distance_matrix[i][j] = 0\n",
    "            else:\n",
    "                distance_matrix[i][j] = distance(barcodes_list[i], barcodes_list[j])\n",
    "    #print distance_matrix\n",
    "    # Then I am going to create a list of indexs\n",
    "    unique_index = []\n",
    "    non_visited_indexes = range(list_length)\n",
    "    # for each element in the index list, I am going remove its duplicates based on the cutoff_value\n",
    "    for i in range(list_length):\n",
    "        if i in non_visited_indexes:\n",
    "            unique_index.append(i)\n",
    "            rm_dup(non_visited_indexes, distance_matrix, i, cutoff_value)\n",
    "    \n",
    "    return unique_index\n",
    "\n",
    "def rm_dup(indexes, distance_matrix, n, cutoff_value):\n",
    "    # this script accept a index list, a distance matrix, and a index n; and then it remove the duplicates of index value n\n",
    "    # and cutoff_value\n",
    "    if n in indexes:\n",
    "        indexes.remove(n)\n",
    "    \n",
    "    all_indexes = indexes[:]\n",
    "    for i in range(len(all_indexes)):\n",
    "        #print all_indexes, indexes, i, n, all_indexes[i]\n",
    "        if all_indexes[i] in indexes:\n",
    "            if (distance_matrix[all_indexes[i], n] <= cutoff_value):\n",
    "                #print i, n, distance_matrix[all_indexes[i], n], indexes\n",
    "                indexes = rm_dup(indexes, distance_matrix, all_indexes[i], cutoff_value)\n",
    "    return indexes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samfile = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    mismatch = sys.argv[3]\n",
    "    rm_dup_samfile(samfile, output_file, mismatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./rm_dup_barcode_UMI.py\n",
    "\n",
    "'''\n",
    "This script accept a input sorted sam file, a output sam file, and a mismatch rate, then it will remove\n",
    "duplicates based on the barcode + UMI (edit distance <= 1), and chromatin and start site, at the same\n",
    "time, it will output the duplication number for each read, and generate the histogram plot for the read\n",
    "per duplication number\n",
    "'''\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "sys.setrecursionlimit(100000)\n",
    "def rm_dup_samfile(samfile, output_file, mismatch):\n",
    "    f1 = open(samfile)\n",
    "    f2 = open(output_file, 'w')\n",
    "    f3 = open(output_file+'.csv', 'w')\n",
    "    pre_barcode = []\n",
    "    pre_line = []\n",
    "    unique_id = []\n",
    "    pre_chrom = 0\n",
    "    pre_site = 0\n",
    "    dup = False\n",
    "    mismatch=int(mismatch)\n",
    "    \n",
    "    pre_dup_num = 0\n",
    "    cur_dup_num = 0\n",
    "    \n",
    "    for line in f1:\n",
    "        \n",
    "        if (line[0] == '@'):\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            name = (((line.split('\\t'))[0]).split(','))\n",
    "            barcode_UMI = name[0] + name[1]\n",
    "            chrom_num = (line.split('\\t'))[2]\n",
    "            start_site = (line.split('\\t'))[3]\n",
    "            \n",
    "            if ((start_site == pre_site) and (chrom_num == pre_chrom)):\n",
    "                if (barcode_UMI in pre_barcode):\n",
    "                    cur_dup_num += 1\n",
    "                else:\n",
    "                    pre_barcode.append(barcode_UMI)\n",
    "                    pre_line.append(line)\n",
    "                '''\n",
    "                dup = False\n",
    "                for each_barcode in pre_barcode:\n",
    "                    edit_dis = distance(each_barcode, barcode_UMI)\n",
    "                    if edit_dis <= mismatch:\n",
    "                        dup = True\n",
    "                        break\n",
    "                \n",
    "                if dup == False:\n",
    "                    pre_dup_num = cur_dup_num\n",
    "                    cur_dup_num = 1\n",
    "                    f2.write(line)\n",
    "                    pre_barcode.add(barcode_UMI)\n",
    "                    f3.write('%d' %(pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "                else:\n",
    "                    cur_dup_num += 1\n",
    "                    if edit_dis == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pre_barcode.add(barcode_UMI)\n",
    "    '''\n",
    "            else:\n",
    "                if (pre_barcode != []):\n",
    "                    unique_id = index_unique(pre_barcode, mismatch)\n",
    "                    for i in unique_id:\n",
    "                        f2.write(pre_line[i])\n",
    "                    cur_dup_num = cur_dup_num + len(pre_barcode) - len(unique_id)\n",
    "                \n",
    "                pre_dup_num = cur_dup_num\n",
    "                cur_dup_num = 1\n",
    "                unique_id = []\n",
    "                pre_barcode = []\n",
    "                pre_line = []\n",
    "                pre_chrom = chrom_num\n",
    "                pre_site = start_site\n",
    "                \n",
    "                pre_barcode.append(barcode_UMI)\n",
    "                pre_line.append(line)\n",
    "                if (pre_dup_num != 0):\n",
    "                    f3.write(\"%d\" % (pre_dup_num))\n",
    "                    f3.write('\\n')\n",
    "    \n",
    "    # also count for the last reads\n",
    "    if (pre_barcode != []):\n",
    "        unique_id = index_unique(pre_barcode, mismatch)\n",
    "        for i in unique_id:\n",
    "            f2.write(pre_line[i])\n",
    "        cur_dup_num = cur_dup_num + len(pre_barcode) - len(unique_id)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    \n",
    "    '''\n",
    "    #plot the histogram for the read duplication number\n",
    "    dups = (pd.read_csv(output_file+'.csv', header=None))[0]\n",
    "    fig = plt.figure()\n",
    "    plt.hist(dups, bins=100)\n",
    "    plt.xlabel(\"Duplication number\")\n",
    "    plt.ylabel(\"Read number\")\n",
    "    fig.savefig(output_file + '.png')\n",
    "    '''\n",
    "\n",
    "def index_unique(barcodes_list, cutoff_value):\n",
    "    # first I am going to create a matrix that include all the pairs of barcodes\n",
    "    list_length = len(barcodes_list)\n",
    "    distance_matrix = np.arange(list_length * list_length).reshape(list_length, list_length)\n",
    "    \n",
    "    for i in range(list_length):\n",
    "        for j in range(list_length):\n",
    "            if (j < i):\n",
    "                distance_matrix[i][j] = distance_matrix[j][i]\n",
    "            elif (j == i):\n",
    "                distance_matrix[i][j] = 0\n",
    "            else:\n",
    "                distance_matrix[i][j] = distance(barcodes_list[i], barcodes_list[j])\n",
    "    #print distance_matrix\n",
    "    # Then I am going to create a list of indexs\n",
    "    unique_index = []\n",
    "    non_visited_indexes = range(list_length)\n",
    "    # for each element in the index list, I am going remove its duplicates based on the cutoff_value\n",
    "    for i in range(list_length):\n",
    "        if i in non_visited_indexes:\n",
    "            unique_index.append(i)\n",
    "            rm_dup(non_visited_indexes, distance_matrix, i, cutoff_value)\n",
    "    \n",
    "    return unique_index\n",
    "\n",
    "def rm_dup(indexes, distance_matrix, n, cutoff_value):\n",
    "    # this script accept a index list, a distance matrix, and a index n; and then it remove the duplicates of index value n\n",
    "    # and cutoff_value\n",
    "    if n in indexes:\n",
    "        indexes.remove(n)\n",
    "    \n",
    "    all_indexes = indexes[:]\n",
    "    for i in range(len(all_indexes)):\n",
    "        #print all_indexes, indexes, i, n, all_indexes[i]\n",
    "        if all_indexes[i] in indexes:\n",
    "            if (distance_matrix[all_indexes[i], n] <= cutoff_value):\n",
    "                #print i, n, distance_matrix[all_indexes[i], n], indexes\n",
    "                indexes = rm_dup(indexes, distance_matrix, all_indexes[i], cutoff_value)\n",
    "    return indexes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samfile = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    mismatch = sys.argv[3]\n",
    "    rm_dup_samfile(samfile, output_file, mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
